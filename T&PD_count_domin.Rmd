---
title: Count Regression Model Dominance Analysis
subtitle: Documentation
#author: Joseph Luchman
author: Anonymized for review
output: html_notebook
editor_options: 
  markdown: 
    wrap: 72
---

This notebook provides documentation for the generation of the data used in the manuscript **Relative Importance Analysis with Count Regression Models**.

The documentation and discussion of approach in this document extends on that provided in the main manuscript as the discussion here focuses more closely on implementation details. These implementation details are are not essential for the reader to understand the manuscript but are essential for understanding the analytic context and for reproducibility.

The code block below initiates the analysis by setting the R environment.

A component of this environment is the random seed to be able to fully reproduce the data. 

Details on the random seed as well as the details of the R environment are available at the end of this notebook.

```{r set-up, results='hide'}
library(gt)
library(knitr)
library(kableExtra)
library(MASS)
library(pscl)
library(tidyverse)
library(magrittr)
library(domir)
library(parameters)
library(datawizard)
library(MCMCpack)

set.seed(84490856)
```

# Data Generation

The manuscript describes four independent variables including:

  a. equipment reliability or _ER_
  b. assistant staffing levels or _AS_
  c. tailor skill-level or _SL_
  d. tailor work experience or _WE_
  
This section provides details related to the population-level parameters used to generate the data used for analysis.

## Generate IV matrix

The generation of the four IVs begins with setting up a population-level covariance matrix describing the variances of and interrelationships between the IVs.

The four IVs were designed such that they would have different variances. The variances chosen were four equally spaced values between standard deviation values of 1 and 2. The first IV, _ER_, was then designed to have a $\sigma_{ER} = 1.00$ and $\sigma^2_{ER} = 1.00$.The second IV, _AS_, was designed to have $\sigma_{AS} = 1.33$ and of $\sigma^2_{IV_2} = 1.78$.  _SL_ was designed to have $\sigma_{SL} = 1.67$ and $\sigma^2_{SL} = 2.78$.  Finally, _WE_ was designed to have $\sigma_{WE} = 2.00$ and $\sigma^2_{WE} = 4.00$.

The design of the interrelationships between the IVs was implemented as to give the IVs non-zero overlaps between one another. This was done as DA is most useful, as a methodology for inferring importance, when the IVs overlap. Although the IVs were designed to overlap, I did not want the extent of the overlap between the IVs to be such that some IVs were more redundant with other IVs than other IVs. All six correlations between the four IVs were thus designed to take on one of three values .250, .125, and .063. These three values were "balanced" across IVs such that the total redundancy for any one IV was identical to the others. 

The six different correlations specified were: $r_{ER,AS} = .250$, $r_{ER,SL} = .125$, $r_{ER,WE} = .063$, $r_{AS,SL} = .063$, $r_{AS,WE} = .125$, and $r_{SL,WE} = .250$.

The manuscript notes that a goal for the generation of the data for analytic use in this exercise is to obtain data that are 'realistic.' The balanced correlations that are determined at the population-level fail to 'feel real.' 

In an effort to add more realism to the dataset, the population covariance matrix is filtered through an inverse Wishart distribution using a single draw and large value (i.e., 100) for the degree of freedom parameter.

Sampling from the inverse Wishart distribution will produce a set of correlation values that are based on the balanced approach but appear more 'as though' generate by a real data generating process. The large value for the degrees of freedom has the effect of reducing the variances of the sampled matrix but also stablizes the correlations of the values sampled to be closer to the population values. The variances of all the variables, once generated in the data, are multiplied by 100 to re-adjust them back to near their original values.

### Population Values

This section sets the population-level covariance matrix.

```{r gen-iv-mat}
namelist <- 
  c("ER", "AS", "SL", "WE")

Covariance_Matrix <- 
  c(1, .5^(2:4)*sqrt(1*c((4/3)^2, (5/3)^2, 4))*sqrt(1),
    .5^2*sqrt(1)*sqrt(1*(4/3)^2), (4/3)^2, .5^(4:3)*sqrt(1*(4/3)^2)*sqrt(1*c((5/3)^2, 4)),
    .5^(3:4)*sqrt(1*(5/3)^2)*sqrt(1*c(1, (4/3)^2)), (5/3)^2, .5^2*sqrt(1*(5/3)^2)*sqrt(1*4),
    .5^(4:2)*sqrt(1*4)*sqrt(1*c(1, (4/3)^2, (5/3)^2)), 4) %>%
  matrix(nrow = 4, byrow = TRUE)

Covariance_Matrix %>% 
  as.data.frame() %>%
  set_names(namelist) %>%
  set_rownames(namelist) %>%
  gt(rownames_to_stub = TRUE) %>%
  tab_header("Table S1: Population Covariance Matrix")

Covariance_Matrix %>% cov2cor() %>% 
  as.data.frame() %>%
  set_names(namelist) %>%  
  set_rownames(namelist) %>%
  gt(rownames_to_stub = TRUE) %>%
  tab_header("Table S2: Population Correlation Matrix")
```

Tables S1 and S2 confirm the correct/intended values are in the covariance and correlation matrices.

... ended here ...

### Sampled Values

The covariance matrix developed doesn't 'feel' like real data.  As such, instead of using it directly, it is used as a parameter in a random draw from an inverse Wishart distribution.

There are a few points of note about the descriptive results from Table X2.  Note the wide variability in the sampled intercorrelations among the four IVs which, in a number of cases for $IV_3$ and $IV_4$ are negative as sampled despite being based on positive correlations in the population.  The standard deviations for the four IVs also show variability from their population values though generally maintain a pattern of $\sigma_{IV_4} > \sigma_{IV_3} > \sigma_{IV_2} > \sigma_{IV_1}$. In addition, the magnitude and direction of several of the correlations may be surprising to the reader given the known coefficient values that produced them.  For example, the correlations between $IV_2$ and all three DVs is negative despite $IV_2$'s known positive coefficient. The negative correlation is due to $IV_2$'s strong, negative correlations with $IV_3$ and $IV_4$ which both have stronger relationships with the DVs.
Correlations reflect a form of \emph{total effect} of an IV on a DV (cite??) and, as will be seen, the biased effect of the correlation does not persist when the model is estimated on the data.
	
The data for use in the DA illustrations are now generated and ready for use in analysis.
DA is a method for extracting additional information from a statistical model such as an LRM.
As such, it should be interpreted in light of the model on which it is based.
The section below estimates the LRM, as well as PR and NBR, CRMs on their respective DVs using all four IVs.

```{r sample-iv-mat}
Covariance_Matrix_Use <- 
  riwish(100, Covariance_Matrix)

Covariance_Matrix_Use %>% 
  as.data.frame() %>%
  gt()

Covariance_Matrix_Use %>% 
  cov2cor() %>% 
  as.data.frame() %>%
  gt()
```


<!-- ### Report -->

<!-- The section below produces the combined correlation matrix tables for display in the manuscript. -->

<!-- ```{r display-covmat} -->
<!-- Covariance_Matrix %>%  -->
<!--   as.data.frame %>% -->
<!--   set_names(str_c("IV", 1:4)) %>% -->
<!--   bind_cols(tibble(name = str_c("IV",1:4))) %>% -->
<!--   bind_rows( -->
<!--     Covariance_Matrix_Use %>% as.data.frame() %>%  -->
<!--       set_names(str_c("IV", 1:4)) %>% -->
<!--       bind_cols(tibble(name = str_c("IV",1:4))) -->
<!--   ) %>% -->
<!--   gt(rowname_col = "name") %>% -->
<!--   tab_row_group(label = "Sampled", rows = 5:8) %>%  -->
<!--   tab_row_group(label = "Parameter", rows = 1:4) %T>% -->
<!--   print %>% -->
<!--   gtsave(filename = "corrs.tex", path = "./includes/") -->
<!-- ``` -->

## Generate IV Data

... adjust the SDs by 10 (which does variance by 100) once generated here ...
... discuss N...


```{r gen-iv-data}
total_n <- 
  runif(n = 1, min = 5000, max = 10000) %>% round()

Data_Frame <- 
  mvrnorm(n = total_n, 
          mu = rep(0, times = 4), 
          Sigma = Covariance_Matrix_Use) %>% 
  as.data.frame() %>%
  rename_with(~ str_c("I", .)) %>%
  multiply_by(10)

Data_Frame %>%
  summarise(
    across(.fns = mean),
    ) 

Data_Frame %>%
  summarise(
    across(.fns = sd),
    ) 

Data_Frame %>% 
  cor %>% 
  kable
```

## Generate Normal Gaussian Dependent Variable

$\beta_{IV_1} =.2$,  $\beta_{IV_2} =.15$,  $\beta_{IV_3} =.12$, $\beta_{IV_4} =.1$. 

	Given that the variance explained for a LRM with no intercorrrelaions can be computed as $R^2_{IV} = \sigma^2_{IV}\beta^2_{IV}$ or the product of the IV's variance and the squared slope, all four IVs have an $R^2$ of .04.

The independent variable data are used to generate the dependent variables starting with the comparison Normally or Gaussian distributed variable.

Similar to the generation of the independent variables, the coefficients for the Normally distributed variable was generated randomly using four draws from a Normal distribution with a means of .2, .15, .125, and .1 and standard deviation of .05.

	The simulated IV values were used to generate the three DVs.  
	The Gaussian/Normally distributed DV was generated first. 
	The coefficient vector implied above was entered as means, into a Normal distribution with a standard deviation of .2 from which a single draw from each mean was obtained.  
	The standard deviation of .2 was intended to make the process of generating coefficients for the DVs less deterministic.  
	The vector of coefficient draws resulted in the following set of values: $\beta_{IV_1} = 0.259$, $\beta_{IV_2} = 0.042$, $\beta_{IV_3} = 0.112$, $\beta_{IV_4} = 0.293$.
	These four values were multiplied by their IV to generate an initial set of Normally distributed DV values.  
	The Normally distributed DV $Y_{Normal}$ was completed by adding a draw from a Normal distribution with a mean of 0 and a standard deviation that was computed as $\sqrt{1 - \sigma^2_{Y_{NormalNoError}}}$ using the initial Normally distributed variable without error.

```{r gen-y-normal}
Coefficient_Vector <- 
  rnorm(4, c(.2, .15, .12, .1), .2)

Coefficient_Vector %>% kable

Data_Frame %<>%
  mutate(
    Y_Normal = 
      IV1*Coefficient_Vector[[1]] + IV2*Coefficient_Vector[[2]] + 
      IV3*Coefficient_Vector[[3]] + IV4*Coefficient_Vector[[4]], 
    Y_Normal = Y_Normal + rnorm(total_n, 0, sqrt(1 - var(Y_Normal)))
  )

Data_Frame %>%
  summarise(
    across(.fns = mean),
    ) 

Data_Frame %>%
  summarise(
    across(.fns = sd),
    ) 

Data_Frame %>% 
  cor %>% 
  kable
```

## Generate Poisson Dependent Variable

The Poisson distributed dependent variable is derived directly from the Normal variable by translating the Normal values to Poisson values by first translating the Normal values to cumulative probabilities and, subsequently, translating those cumulative probabilities to Poisson quantiles assuming a mean of 1.

	A Poisson distributed version of $Y_{Normal}$, $Y_{Poisson}$, was generated by obtaining the cumulative probability of each observation's $Y_{Normal}$ score given the standard Gaussian/Normal distribution (i.e., $N(0,1)$) and translating that value into a count value using the Poisson quantile function assuming an underlying mean and variance of 1.  
	The mean of 1 value was chosen to make the Poisson distributed result as close to the Normally distributed result as possible as both have variances of 1.	

```{r gen-y-poisson}
Data_Frame %<>%
  mutate(
    Y_Poisson = qpois(pnorm(Y_Normal, log.p = TRUE), 1, log.p = TRUE)
  )

Data_Frame %>%
  summarise(
    across(.fns = mean),
    ) 

Data_Frame %>%
  summarise(
    across(.fns = sd),
    ) 

Data_Frame %>% 
  cor %>% 
  kable
```

## Generate Negative Binomial Dependent Variable

The generation of the negative Binomial dependent variable followed the same procedure as that to create the Poisson variable as it is created from the cumulative probability derived from the Normally distributed variable translated into negative Binomial quantiles assuming a _size_ of .5 and _mu_ parameter of 1.

The _size_ and _mu_ parameters were chosen to obtain a mean (i.e., the _mu_/$\mu$ value) of 1 to mirror the Poisson distribution but a variance that substantially exceeded 1; in this case  or a value of 3 as $\sigma^{2} = \mu + \frac{\mu^2}{size}$.

	A negative Binomial version, $Y_{NegativeBinomial}$, was created similarly by translating $Y_{Normal}$'s cumulative probability for each observation into a count variable using the negative Binomial quantile function assuming a Gamma shape parameter of .5 and scale parameter of .33; these factors result in a mean of 1 and variance of 3 for the negative Binomial distribution.  
	The negative Binomial parameters obtaining a mean of 1 and variance of 3 were chosen to maintain similarities to the Poisson version of the variable (i.e., their means are identical) but to emphasize the unique feature of the negative Binomial distribution (i.e., the capability to model over-dispersion). 

```{r gen-y-neg-bin}
Data_Frame %<>%
  mutate(
    Y_NB = qnbinom(pnorm(Y_Normal, log.p = TRUE), size = .5, mu = 1, log.p = TRUE),
  )

Data_Frame %>%
  summarise(
    across(.fns = mean),
    )

Data_Frame %>%
  summarise(
    across(.fns = sd),
    )

Data_Frame %>%
  cor %>%
  kable
```

... display later maybe? ...

## Report Generated Data Descriptives

```{r display-observed-descriptives}
Data_Frame %>%
  {
    cbind(
      summarise(.,
        across(.fns = mean)
      ) %>% t(),
      summarise(.,
        across(.fns = sd)
      ) %>% t(),
      cor(.) %>% as.data.frame()
      
    )
  } %>% 
  set_names(c("Means", "SDs", str_c("IV", 1:4), "Y_Poisson", "Y_NB")) %>%
  as_tibble(rownames = "IVs") %>%
  gt(rowname_col = "IVs") %>%
  tab_style(
    style = cell_borders(sides = "right"),
    locations = cells_body(columns = "SDs")
  ) %T>%
  print %>%
  gtsave(filename = "descrips_initial.tex", path = "./includes/")
```

## Poisson with Offset Dependent Variables

The dependent variables with an offset were generated assuming a Poisson distribution and two different versions of the Poisson-with-offset variables were created.

### Uncorrelated Offset

The Poisson variable with an offset is generated from a binomial distribution with the idea that the Poisson rate/mean parameter can be derived as $np = \lambda$.  In this case, the _n_ is 10 and the _p_ is .1 to obtain the mean of 1 (i.e., $10*.1 = 1$).

Given there are up to 10 trials, the offset is implemented as the number of trials actually realized for an observation.  This number of trials is obtained as an integer vector from 1 to 10 sampled randomly for all observations.

The number of 'successes' (i.e., the Poisson variable) is then implemented, as before, using the cumulative probability from the Normal variable as translated to a binomial quantile using the number of realized trials for each observation and a fixed probability of .1).

```{r offset-uncorrelated}
Data_Frame %<>%
  mutate(
    Number_Observed = sample(1:10, total_n, replace = TRUE)
  )

Data_Frame %>% select(Number_Observed) %>% 
  ggplot(aes(Number_Observed)) + stat_bin()

Data_Frame %<>%
  mutate(
    Y_Poisson_Offset = 
      qbinom(
        pnorm(Y_Normal, log.p = TRUE),
             Number_Observed, .1, log.p = TRUE
        )
  )

Data_Frame %>%
  select(Y_Poisson_Offset) %>%
  ggplot(aes(Y_Poisson_Offset)) + stat_bin()

Data_Frame %>%
  summarise(
    across(.fns = mean),
    ) 

Data_Frame %>%
  summarise(
    across(.fns = sd),
    ) 

Data_Frame %>% 
  cor %>% 
  kable

```

### Correlated Offset

The correlated offset variable is implemented in a way similar to the uncorrelated offset variable save for the way in which the number of realized trials was implemented for each observation.  

The number of realized trials was obtained by randomly sampling coefficients in the same way as for the development of the Normally distributed variable (4 draws from Normal distribution with the same set of means as the Normally distributed variable but randomized in order and negative and a standard deviation of .05) and generating a Normally distributed variable with a standard deviation of 1.  The newly generated Normally distributed variable is then translated into a cumulative probability and uniform distribution quantile with a minimum of 1 and maximum of 10.  

This new 1 to 10 count variable is then used as the number of realized trials in the same process used to generate an offset as was implemented above.

```{r correlated-offset}
Offset_Vector <- 
  rnorm(4, sample(c(.2, .15, .12, .1))*-1, .2)

Offset_Vector %>% kable

Data_Frame %<>%
  mutate(
    Censoring =
      IV1*Offset_Vector[[1]] + IV2*Offset_Vector[[2]] + 
      IV3*Offset_Vector[[3]] + IV4*Offset_Vector[[4]],
    Censoring = Censoring + rnorm(total_n, 0, sqrt(1 - var(Censoring)))
  )

Data_Frame %<>%
  mutate(
    N_obs_censor =
      qunif(
        pnorm(Censoring, log.p = TRUE), 
        1, 10, log.p = TRUE) %>% ceiling(),
    Y_Poisson_Censored = 
      qbinom(
        pnorm(Y_Normal, log.p = TRUE),
             N_obs_censor, .1, log.p = TRUE
        )
  )

Data_Frame %>% select(N_obs_censor) %>% 
  ggplot(aes(N_obs_censor)) + stat_bin()

Data_Frame %>%
  select(Y_Poisson_Censored) %>%
  ggplot(aes(Y_Poisson_Censored)) + stat_bin()

Data_Frame %>%
  summarise(
    across(.fns = mean),
    ) 

Data_Frame %>%
  summarise(
    across(.fns = sd),
    ) 

Data_Frame %>% 
  cor %>% 
  kable
```

## Zero-inflated Poisson Dependent Variable

The zero-inflated Poisson dependent variable uses the Poisson variable as well as the Normally distributed variable created to produce the correlated offset (negative, randomized Normal variable coefficients).  The correlated offset variable is translated through a cumulative Normal probability to a binomial quantile function with 1 draw and a probability of .9. 


```{r zero-inflate}
Data_Frame %<>%
  mutate(
    Y_ZIP = 
      Y_Poisson * 
      qbinom(
        pnorm(Censoring, log.p = TRUE),
        1, .9, log.p = TRUE)
  )

Data_Frame %>%
  select(Y_ZIP) %>%
  ggplot(aes(Y_ZIP)) + stat_bin()

Data_Frame %>%
  summarise(
    across(.fns = mean),
    ) 

Data_Frame %>%
  summarise(
    across(.fns = sd),
    ) 

Data_Frame %>% 
  cor %>% 
  kable

```

# Primary Modeling

## Poisson Regressions

### Obtain Modeling Results

```{r poisson-result}
poisson_result <- 
  glm(Y_Poisson ~ IV1 + IV2 + IV3 + IV4, data = Data_Frame, family = poisson()) 

poisson_unstzd_params <- 
  poisson_result %>% 
  parameters() 

poisson_stdzd_params <- 
  poisson_result %>%
  standardise_parameters(method = "basic")

baseline_model <- 
  glm(Y_Poisson ~ 1, data = Data_Frame, family = poisson())
  
dev_r2 <-
  function(obj) {
    res <- (1 - obj$deviance/baseline_model$deviance)
    names(res) <- "dev_r2"
    return(res)
  }

poisson_da <-
  domir(Y_Poisson ~ IV1 + IV2 + IV3 + IV4, 
        \(fml, ...) {
          res <- 
            glm(fml, ...) 
          return(dev_r2(res))
        },
        data = Data_Frame, family = poisson())

poisson_unstzd_params %>% 
  display(digits = 3)

poisson_stdzd_params %>%
  display(digits = 3)

poisson_da %>%
  print(digits = 3)

```

### Obtain All Subsets R-square

```{r poisson-all-subsets}
baseline_model <- 
  glm(Y_Poisson ~ 1, data = Data_Frame, family = poisson())

dev_r2 <-
  function(obj) {
    res <- list(1 - obj$deviance/baseline_model$deviance)
    names(res) <- "dev_r2"
    return(res)
  }

ps_capture <- 
  function(formula, ...) { # wrapper program that accepts formula and ellipsis arguments
    count <<- count + 1 # increment counter in enclosing environment
    ps_obj <- glm(formula, ...) # estimate 'glm' model and save object
    DA_ps_results[count, "formula"] <<- 
      deparse(formula) # record string version of formula passed in 'DA_results' in enclosing environment
    DA_ps_results[count, "R^2"] <<- 
      #r2(ps_obj)[["R2_Nagelkerke"]] # record R^2 in 'DA_results' in enclosing environment
      1 - ps_obj$deviance/baseline_model$deviance
    return(ps_obj) # return 'lm' class-ed object
  }

count <- 0 # initialize the count indicating the row in which the results will fill-in

DA_ps_results <- # container data frame in which to record results
  data.frame(formula = rep("", times = 2^3-1), 
             `R^2` = rep(NA, times = 2^3-1), 
             check.names = FALSE)

ps_da <- domin(Y_Poisson ~ IV1 + IV2 + IV3 + IV4, # implement the DA with the wrapper
               ps_capture, 
               #list(r2, "R2_Nagelkerke"), 
               list(dev_r2, "dev_r2"), 
               data = Data_Frame, family = poisson())

DA_ps_results
```

### Report Coefficients

```{r display-poisson-coeffs}
poisson_unstzd_params %>% 
  select(Parameter, Coefficient, SE, z, p, CI_low, CI_high) %>%
  left_join(
    poisson_stdzd_params %>%
      select(Parameter, Std_Coefficient),
    by = "Parameter"
  ) %>%
  gt(rowname_col = "Parameter") %>%
  fmt_number(
    columns = c("Coefficient", "SE", "z", "p", "CI_low", "CI_high", 
                "Std_Coefficient"),
    decimals = 3
  ) %T>%
  print %>%
  gtsave(filename = "poisson_coeff.tex", path = "./includes/")
```

## Negative Binomial Regressions

### Obtain Modeling Results

```{r NB-result}
nb_result <-
  glm.nb(Y_NB ~ IV1 + IV2 + IV3 + IV4, data = Data_Frame)

nb_result %>%
  parameters()

nb_result %>%
  standardise_parameters(method = "basic")

# ~~~ NB regs don't work with dominance_analysis() - method dispatch issue?
# ~~~ they have r2() method but DA doesn't recognize it
# nb_result %>%
#   dominance_analysis()
domir(Y_NegativeBinomial ~ IV1 + IV2 + IV3 + IV4,
      \(fml,...) {
        res <- glm.nb(fml, ...)
        ret <- r2(res)[[1]]
      }
      ,
      data = Data_Frame)
```



```{r NB-all-subsets}
nb_capture <-
  function(formula, ...) { # wrapper program that accepts formula and ellipsis arguments
    count <<- count + 1 # increment counter in enclosing environment
    nb_obj <- glm.nb(formula, ...) # estimate 'glm' model and save object
    DA_nb_results[count, "formula"] <<-
      deparse(formula) # record string version of formula passed in 'DA_results' in enclosing environment
    DA_nb_results[count, "R^2"] <<-
      r2(nb_obj)[["R2_Nagelkerke"]] # record R^2 in 'DA_results' in enclosing environment
    return(nb_obj) # return 'lm' class-ed object
  }

count <- 0 # initialize the count indicating the row in which the results will fill-in

DA_nb_results <- # container data frame in which to record results
  data.frame(formula = rep("", times = 2^3-1),
             `R^2` = rep(NA, times = 2^3-1),
             check.names = FALSE)

nb_da <- domin(Y_NegativeBinomial ~ IV1 + IV2 + IV3 + IV4, # implement the DA with the wrapper
               nb_capture,
               list(r2, "R2_Nagelkerke"),
               data = Data_Frame)

DA_nb_results
```


## Combined General Dominance Reporting


```{r gend-dom-display}
lm_da %>%
  pluck(1) %>%
  filter(Parameter != "(Intercept)") %>%
  select(-Subset) %>%
  bind_rows(
    poisson_da %>%
      pluck(1) %>%
      as_tibble(rownames = "Parameter") %>%
      rename(`General_Dominance` = "value") %>%
      left_join(
        poisson_da %>%
          pluck(2) %>%
          as_tibble(rownames = "Parameter") %>%
          rename(`Percent` = "value")
      ) %>%
      left_join(
        poisson_da %>%
          pluck(3) %>%
          as_tibble(rownames = "Parameter") %>%
          rename(`Ranks` = "value")
      ) 
    ) %>%
  gt(rowname_col = "Parameter") %>%
  tab_row_group(label = "Poisson", rows = 5:8) %>% 
  tab_row_group(label = "Normal", rows = 1:4) %>%
  fmt_number(
    columns = c("General_Dominance", "Percent"),
    decimals = 4
  ) %T>%
  print %>%
  gtsave(filename = "gen_dom.tex", path = "./includes/")
```

## Conditional Dominance Graphic Reporting

```{r condit-domin-display}
lm_da %>%
  pluck(2) %>%
  mutate(Model = "Normal") %>%
  bind_rows(
    poisson_da$Conditional_Dominance %>%
      as_tibble(rownames = "Subset") %>%
      rename_with(
        .fn = ~ str_replace_all(., "subset_size", "IVs"),
        .cols = starts_with("subset")
      ) %>%
      mutate(Model = "Poisson")
  ) %>%
  pivot_longer(
    cols = starts_with("IVs"),
    names_to = "IVs",
    values_to = "R2",
    names_prefix = "IVs_"
    ) %>%
  ggplot(
    aes(x = IVs, y = R2, group = Subset, linetype = Subset)
  ) + geom_line() + 
  facet_grid(rows = vars(Model)) + theme_linedraw()

ggsave("condit_gph.png", height = 4, width = 6, 
       units = "in", path = "./includes/")
```

# Offset Modeling

## Wrong Poisson

```{r}
poisson_result_offset_wrong <- 
  glm(Y_Poisson_Offset ~ IV1 + IV2 + IV3 + IV4, data = Data_Frame, 
      family = poisson())

poisson_result_offset_wrong %>% 
  parameters()

poisson_result_offset_wrong %>%
  standardise_parameters(method = "basic")

domir(Y_Poisson_Offset ~ IV1 + IV2 + IV3 + IV4, 
      \(fml,...) {
        res <- glm(fml, ...) 
        ret <- r2(res)[[1]]
      }
      ,
      data = Data_Frame, family = poisson()) 
```

## Right Poisson

```{r}
poisson_result_offset_right <- 
  glm(Y_Poisson_Offset ~IV1 + IV2 + IV3 + IV4 + offset(log(Number_Observed)), data = Data_Frame, 
      family = poisson())

poisson_result_offset_right %>% 
  parameters()

poisson_result_offset_right %>%
  standardise_parameters(method = "basic")

domir(Y_Poisson_Offset ~ IV1 + IV2 + IV3 + IV4, 
      \(fml,...) {
        fml2 <- update(fml, . ~ . + offset(log(Number_Observed)))
        res <- glm(fml2, ...) 
        ret <- r2(res)[[1]]
      }
      ,
      data = Data_Frame, family = poisson()) 
```



## Wrong Poisson C

```{r}
poisson_result_cens_wrong <- 
  glm(Y_Poisson_Censored ~ IV1 + IV2 + IV3 + IV4, data = Data_Frame, 
      family = poisson())

poisson_result_cens_wrong %>% 
  parameters()

poisson_result_cens_wrong %>%
  standardise_parameters(method = "basic")

domir(Y_Poisson_Censored ~ IV1 + IV2 + IV3 + IV4, 
      \(fml,...) {
        res <- glm(fml, ...) 
        ret <- r2(res)[[1]]
      }
      ,
      data = Data_Frame, family = poisson()) 
```

## Right Poisson C

```{r}
poisson_result_cens_right <- 
  glm(Y_Poisson_Censored ~IV1 + IV2 + IV3 + IV4 + offset(log(N_obs_censor)), data = Data_Frame, 
      family = poisson())

poisson_result_cens_right %>% 
  parameters()

poisson_result_cens_right %>%
  standardise_parameters(method = "basic")

domir(Y_Poisson_Censored ~ IV1 + IV2 + IV3 + IV4, 
      \(fml,...) {
        fml2 <- update(fml, . ~ . + offset(log(N_obs_censor)))
        res <- glm(fml2, ...) 
        ret <- r2(res)[[1]]
      }
      ,
      data = Data_Frame, family = poisson()) 
```

# Zero-Inflation

```{r}
zip_result <- 
  zeroinfl(Y_ZIP ~ IV1 + IV2 + IV3 + IV4 | IV1 + IV2 + IV3 + IV4, 
           data = Data_Frame)

zip_result %>% 
  parameters()

zip_result %>%
  standardise_parameters(method = "basic")
```

## IV DA

```{r}
domir(Y_ZIP ~ IV1 + IV2 + IV3 + IV4, 
      \(fml,...) {
        fml2 <- update(fml, . ~ .)
        res <- zeroinfl(fml2, ...) 
        ret <- r2(res)[[1]]
      }
      ,
      data = Data_Frame) 
```

# Records and Reproducibility

```{r}
sessionInfo()
```

The random seed information used in this session:

    84490856
    Min: 1, Max: 99999999
    2022-08-14 14:50:02 UTC
    Random.org
